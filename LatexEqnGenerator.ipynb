{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMQxkQuvLs6VMT+PDo4SBOC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BARKHAC/HandwrittenEqn-LatexConverter/blob/main/LatexEqnGenerator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5127h9RvLM8"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h60grUD-Sr52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets jiwer"
      ],
      "metadata": {
        "id": "nZCYMcU_vi-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KrT2xRxovofD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -R /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "FUpFjacKzHKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/5k_dataset_new.zip"
      ],
      "metadata": {
        "collapsed": true,
        "id": "lhtuua4SzgwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "from evaluate import load as load_metric\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Rh__J75gzt4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "u6RvIGz4Gfu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    # pred is an object with 'predictions' and 'label_ids'\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # Decode predictions using our custom tokenizer\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in labels with the pad token id\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
        "    return {\"cer\": cer}\n"
      ],
      "metadata": {
        "id": "lO4LBMD2HZjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from transformers import ViTFeatureExtractor, PreTrainedTokenizerFast\n",
        "\n",
        "class HandwrittenEquationDataset(Dataset):\n",
        "    def __init__(self, img_dir, label_dir, tokenizer_path, processor_path, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.transform = transform or transforms.ToTensor()  # Default to ToTensor()\n",
        "        # Load ViTFeatureExtractor for image processing\n",
        "        self.feature_extractor = ViTFeatureExtractor.from_pretrained(processor_path)\n",
        "\n",
        "        # Load tokenizer\n",
        "        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)\n",
        "        self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "        # Get all image filenames ending with .png\n",
        "        self.image_files = sorted([f for f in os.listdir(img_dir) if f.endswith('.png')])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_filename = self.image_files[idx]\n",
        "        img_path = os.path.join(self.img_dir, img_filename)\n",
        "\n",
        "        # Open image and convert to RGB\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Process the image using the processor (resizing, normalizing, etc.)\n",
        "        pixel_values = self.feature_extractor(image, return_tensors=\"pt\").pixel_values.squeeze(0)\n",
        "\n",
        "        # Get corresponding label filename\n",
        "        base_name = img_filename.replace('.inkml.png', '')\n",
        "        label_filename = base_name + '.txt'\n",
        "        label_path = os.path.join(self.label_dir, label_filename)\n",
        "\n",
        "        # Load label\n",
        "        with open(label_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            label_text = f.read().strip()\n",
        "            # Tokenize label\n",
        "        tokenized_label = self.tokenizer(label_text, return_tensors=\"pt\")\n",
        "\n",
        "        return {\"pixel_values\": pixel_values, \"labels\": tokenized_label[\"input_ids\"].squeeze(0)}\n",
        "\n",
        "\n",
        "def collate_fn(batch, tokenizer):\n",
        "    # Extract images and labels from the list of dictionaries\n",
        "    images = [item[\"pixel_values\"] for item in batch]\n",
        "    labels = [item[\"labels\"] for item in batch]\n",
        "\n",
        "    # Convert images to tensor\n",
        "    images = torch.stack(images)\n",
        "\n",
        "    # Print raw tokenized labels before padding\n",
        "    print(\"Raw tokenized labels:\", labels)\n",
        "\n",
        "    # Find the longest sequence in this batch\n",
        "    max_len = max(label.shape[0] for label in labels)\n",
        "    print(f\"Max label length in batch: {max_len}\")\n",
        "\n",
        "    # Pad labels dynamically\n",
        "    padded_labels = torch.nn.utils.rnn.pad_sequence(\n",
        "        labels, batch_first=True, padding_value=-100\n",
        "    )\n",
        "\n",
        "    return images, padded_labels\n",
        "\n",
        "# Example usage\n",
        "tokenizer_path = \"\"\n",
        "dataset = HandwrittenEquationDataset(\n",
        "    img_dir=\"/content/5k_dataset_new/images_5k_new\",\n",
        "    label_dir=\"/content/5k_dataset_new/labels_5k_new\",\n",
        "    tokenizer_path=\"/content/dataset_5k/latex_tokenizer_sentencepiece\",\n",
        "    processor_path=\"microsoft/trocr-base-stage1\"  # Uses only the ViT feature extractor\n",
        ")\n",
        "\n",
        "\n",
        "# Create DataLoader with custom collate function\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=lambda b: collate_fn(b, dataset.tokenizer))\n",
        "\n",
        "# Fetch a batch and verify\n",
        "images, tokenized_labels = next(iter(dataloader))\n",
        "print(f\"Batch images shape: {images.shape}\")  # Should be (batch_size, 1, H, W)\n",
        "print(f\"Batch tokenized labels shape: {tokenized_labels.shape}\")\n",
        "print(f\"Sample tokenized label: {tokenized_labels[0]}\")\n"
      ],
      "metadata": {
        "id": "OKro7eKkIufp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2nd March 2025: New DataLoader(basically same but with loading tokenizer from saved model at LatexModel)"
      ],
      "metadata": {
        "id": "ii7YAVOCPDlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from transformers import ViTFeatureExtractor, PreTrainedTokenizerFast\n",
        "\n",
        "class HandwrittenEquationDataset(Dataset):\n",
        "    def __init__(self, img_dir, label_dir, tokenizer_path, processor_path, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.transform = transform or transforms.ToTensor()  # Default to ToTensor()\n",
        "        # Load ViTFeatureExtractor for image processing\n",
        "        self.feature_extractor = ViTFeatureExtractor.from_pretrained(processor_path)\n",
        "\n",
        "        # Load tokenizer from the pretrained model folder (LatexModel)\n",
        "        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)\n",
        "        self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "        # Get all image filenames ending with .png\n",
        "        self.image_files = sorted([f for f in os.listdir(img_dir) if f.endswith('.png')])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_filename = self.image_files[idx]\n",
        "        img_path = os.path.join(self.img_dir, img_filename)\n",
        "\n",
        "        # Open image and convert to RGB\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Process the image using the feature extractor (resizing, normalizing, etc.)\n",
        "        pixel_values = self.feature_extractor(image, return_tensors=\"pt\").pixel_values.squeeze(0)\n",
        "\n",
        "        # Get corresponding label filename\n",
        "        base_name = img_filename.replace('.inkml.png', '')\n",
        "        label_filename = base_name + '.txt'\n",
        "        label_path = os.path.join(self.label_dir, label_filename)\n",
        "\n",
        "        # Load label and tokenize\n",
        "        with open(label_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            label_text = f.read().strip()\n",
        "        tokenized_label = self.tokenizer(label_text, return_tensors=\"pt\")\n",
        "\n",
        "        return {\"pixel_values\": pixel_values, \"labels\": tokenized_label[\"input_ids\"].squeeze(0)}\n",
        "\n",
        "\n",
        "def collate_fn(batch, tokenizer):\n",
        "    # Extract images and labels from the list of dictionaries\n",
        "    images = [item[\"pixel_values\"] for item in batch]\n",
        "    labels = [item[\"labels\"] for item in batch]\n",
        "\n",
        "    # Convert images to tensor\n",
        "    images = torch.stack(images)\n",
        "\n",
        "    # Print raw tokenized labels before padding\n",
        "    print(\"Raw tokenized labels:\", labels)\n",
        "\n",
        "    # Find the longest sequence in this batch\n",
        "    max_len = max(label.shape[0] for label in labels)\n",
        "    print(f\"Max label length in batch: {max_len}\")\n",
        "\n",
        "    # Pad labels dynamically; using -100 as the padding value for labels\n",
        "    padded_labels = torch.nn.utils.rnn.pad_sequence(\n",
        "        labels, batch_first=True, padding_value=-100\n",
        "    )\n",
        "\n",
        "    return images, padded_labels\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# Set the tokenizer_path to point to your saved model (LatexModel) folder.\n",
        "tokenizer_path = \"/content/drive/MyDrive/LatexModel\"  # <-- Updated to your model folder\n",
        "processor_path = \"microsoft/trocr-base-stage1\"         # Using the ViT feature extractor from TroCR\n",
        "\n",
        "dataset = HandwrittenEquationDataset(\n",
        "    img_dir=\"/content/5k_dataset_new/images_5k_new\",\n",
        "    label_dir=\"/content/5k_dataset_new/labels_5k_new\",\n",
        "    tokenizer_path=tokenizer_path,\n",
        "    processor_path=processor_path\n",
        ")\n",
        "\n",
        "# Create DataLoader with the custom collate function\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: collate_fn(batch, dataset.tokenizer)\n",
        ")\n",
        "\n",
        "# Fetch a batch and verify\n",
        "images, tokenized_labels = next(iter(dataloader))\n",
        "print(f\"Batch images shape: {images.shape}\")          # Should be (batch_size, C, H, W)\n",
        "print(f\"Batch tokenized labels shape: {tokenized_labels.shape}\")\n",
        "print(f\"Sample tokenized label: {tokenized_labels[0]}\")\n"
      ],
      "metadata": {
        "id": "Sg4qG4y6PMiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2nd March 2025: New Training Snippet (basically same we are just loading the model from LatexModel)"
      ],
      "metadata": {
        "id": "QDig3oMCP1X0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from transformers import (\n",
        "    VisionEncoderDecoderModel,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    PreTrainedTokenizerFast,\n",
        "    ViTFeatureExtractor\n",
        ")\n",
        "from evaluate import load as load_metric\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define checkpoint directory\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/trocr_checkpoints\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# Use the saved model from LatexModel folder (which also contains the tokenizer and preprocessor config)\n",
        "MODEL_PATH = \"/content/drive/MyDrive/LatexModel\"  # Path to your saved model folder\n",
        "TOKENIZER_PATH = MODEL_PATH  # Use the same folder for the tokenizer\n",
        "PROCESSOR_PATH = MODEL_PATH  # Use the preprocessor configuration from the LatexModel folder\n",
        "\n",
        "# Load tokenizer from LatexModel folder\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(TOKENIZER_PATH)\n",
        "# (Optional) Add pad token if needed\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# Load the model from LatexModel folder\n",
        "model = VisionEncoderDecoderModel.from_pretrained(MODEL_PATH)\n",
        "model.config.pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 1\n",
        "model.config.decoder.pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 1\n",
        "if model.config.decoder_start_token_id is None:\n",
        "    model.config.decoder_start_token_id = 2\n",
        "\n",
        "# Patch the encoder's forward function to remove an unexpected keyword argument.\n",
        "original_encoder_forward = model.encoder.forward\n",
        "def patched_encoder_forward(*args, **kwargs):\n",
        "    kwargs.pop(\"num_items_in_batch\", None)\n",
        "    return original_encoder_forward(*args, **kwargs)\n",
        "model.encoder.forward = patched_encoder_forward\n",
        "\n",
        "# Define the dataset class inline\n",
        "class HandwrittenEquationDataset(Dataset):\n",
        "    def __init__(self, img_dir, label_dir, tokenizer_path, processor_path, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.transform = transform or transforms.ToTensor()  # Default to ToTensor()\n",
        "        # Load ViTFeatureExtractor for image processing using the preprocessor config in LatexModel folder\n",
        "        self.feature_extractor = ViTFeatureExtractor.from_pretrained(processor_path)\n",
        "\n",
        "        # Load tokenizer from the pretrained model folder (LatexModel)\n",
        "        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)\n",
        "        self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "        # Get all image filenames ending with .png\n",
        "        self.image_files = sorted([f for f in os.listdir(img_dir) if f.endswith('.png')])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_filename = self.image_files[idx]\n",
        "        img_path = os.path.join(self.img_dir, img_filename)\n",
        "\n",
        "        # Open image and convert to RGB\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Process the image using the feature extractor (resizing, normalizing, etc.)\n",
        "        pixel_values = self.feature_extractor(image, return_tensors=\"pt\").pixel_values.squeeze(0)\n",
        "\n",
        "        # Get corresponding label filename\n",
        "        base_name = img_filename.replace('.inkml.png', '')\n",
        "        label_filename = base_name + '.txt'\n",
        "        label_path = os.path.join(self.label_dir, label_filename)\n",
        "\n",
        "        # Load label and tokenize\n",
        "        with open(label_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            label_text = f.read().strip()\n",
        "        tokenized_label = self.tokenizer(label_text, return_tensors=\"pt\")\n",
        "\n",
        "        return {\"pixel_values\": pixel_values, \"labels\": tokenized_label[\"input_ids\"].squeeze(0)}\n",
        "\n",
        "# Define a custom collate function\n",
        "def custom_data_collator(features):\n",
        "    # Stack pixel_values (they are already tensors)\n",
        "    pixel_values = torch.stack([f[\"pixel_values\"] for f in features])\n",
        "    # Pad labels dynamically using pad_sequence, using -100 as the pad value for labels\n",
        "    labels = [f[\"labels\"] for f in features]\n",
        "    padded_labels = torch.nn.utils.rnn.pad_sequence(\n",
        "        labels, batch_first=True, padding_value=-100\n",
        "    )\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": padded_labels}\n",
        "\n",
        "# (Optional) A wrapper that removes extra keys if needed\n",
        "def data_collator_wrapper(features):\n",
        "    batch = custom_data_collator(features)\n",
        "    batch.pop(\"num_items_in_batch\", None)\n",
        "    return batch\n",
        "\n",
        "# (Optional) A simple compute_metrics function\n",
        "def compute_metrics(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "    # If pred_ids is a tuple, select the first element\n",
        "    if isinstance(pred_ids, tuple):\n",
        "        pred_ids = pred_ids[0]\n",
        "    # Replace -100 with the pad token id\n",
        "    pred_ids[pred_ids == -100] = tokenizer.pad_token_id\n",
        "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "    # Calculate simple exact match accuracy\n",
        "    exact_match = sum([1 if p.strip() == l.strip() else 0 for p, l in zip(pred_str, label_str)]) / len(pred_str)\n",
        "    return {\"exact_match\": exact_match}\n",
        "\n",
        "# Create the dataset using the new images and labels directories.\n",
        "dataset = HandwrittenEquationDataset(\n",
        "    img_dir=\"/content/5k_dataset_new/images_5k_new\",\n",
        "    label_dir=\"/content/5k_dataset_new/labels_5k_new\",\n",
        "    tokenizer_path=TOKENIZER_PATH,\n",
        "    processor_path=PROCESSOR_PATH\n",
        ")\n",
        "\n",
        "# Split dataset into train and eval sets (e.g., 85% train, 15% eval)\n",
        "train_size = int(0.85 * len(dataset))\n",
        "eval_size = len(dataset) - train_size\n",
        "train_dataset, eval_dataset = random_split(dataset, [train_size, eval_size])\n",
        "print(f\"Train size: {len(train_dataset)}, Eval size: {len(eval_dataset)}\")\n",
        "\n",
        "# Custom Trainer that filters out extra keys if needed.\n",
        "class MySeq2SeqTrainer(Seq2SeqTrainer):\n",
        "    def _prepare_inputs_for_model(self, inputs):\n",
        "        allowed_keys = {\"pixel_values\", \"labels\"}\n",
        "        filtered_inputs = {k: v for k, v in inputs.items() if k in allowed_keys}\n",
        "        return super()._prepare_inputs_for_model(filtered_inputs)\n",
        "\n",
        "# Training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=CHECKPOINT_DIR,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    fp16=True,\n",
        "    num_train_epochs=5,\n",
        "    logging_steps=500,\n",
        "    evaluation_strategy=\"no\",  # Disable eval during training to reduce VRAM usage\n",
        "    save_steps=1500,\n",
        "    save_strategy=\"steps\",\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=False,\n",
        "    metric_for_best_model=\"loss\",\n",
        "    greater_is_better=False,\n",
        "    report_to=\"none\",\n",
        "    gradient_checkpointing=True\n",
        ")\n",
        "\n",
        "# Initialize the Trainer with the custom data collator and trainer class.\n",
        "trainer = MySeq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=custom_data_collator,  # You can also use data_collator_wrapper if preferred.\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Check for an existing checkpoint (if any)\n",
        "checkpoint_paths = glob.glob(f\"{CHECKPOINT_DIR}/checkpoint-*\")\n",
        "checkpoint_path = max(checkpoint_paths, key=os.path.getctime) if checkpoint_paths else None\n",
        "if checkpoint_path:\n",
        "    print(f\"Resuming from checkpoint: {checkpoint_path}\")\n",
        "else:\n",
        "    print(\"No checkpoint found. Starting training from scratch.\")\n",
        "\n",
        "# Start training (resuming from checkpoint if available)\n",
        "trainer.train(resume_from_checkpoint=checkpoint_path if checkpoint_path else None)\n"
      ],
      "metadata": {
        "id": "WUsn-lYvP_-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from transformers import VisionEncoderDecoderModel, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from evaluate import load as load_metric\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define checkpoint directory\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/trocr_checkpoints\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# Load tokenizer\n",
        "TOKENIZER_PATH = \"/content/dataset_5k/latex_tokenizer_sentencepiece\"\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(TOKENIZER_PATH)\n",
        "\n",
        "# (Optional: if the tokenizer already has a pad token, you don't need to add one)\n",
        "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# Load TrOCR model\n",
        "MODEL_NAME = \"microsoft/trocr-base-stage1\"\n",
        "model = VisionEncoderDecoderModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "\n",
        "\n",
        "model.config.pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 1\n",
        "model.config.decoder.pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 1\n",
        "\n",
        "\n",
        "if model.config.decoder_start_token_id is None:\n",
        "    model.config.decoder_start_token_id = 2\n",
        "\n",
        "original_encoder_forward = model.encoder.forward\n",
        "def patched_encoder_forward(*args, **kwargs):\n",
        "    kwargs.pop(\"num_items_in_batch\", None)\n",
        "    return original_encoder_forward(*args, **kwargs)\n",
        "model.encoder.forward = patched_encoder_forward\n",
        "\n",
        "# Assume HandwrittenEquationDataset is defined as before and returns a dictionary:\n",
        "# {\"pixel_values\": image, \"labels\": tokenized_label}\n",
        "dataset = HandwrittenEquationDataset(\n",
        "    img_dir=\"/content/dataset_5k/images_5k\",\n",
        "    label_dir=\"/content/dataset_5k/labels_5k\",\n",
        "    tokenizer_path=TOKENIZER_PATH,\n",
        "    processor_path=\"microsoft/trocr-base-stage1\"  # Uses only the ViT feature extractor\n",
        ")\n",
        "\n",
        "# Split dataset into train and eval sets\n",
        "train_size = int(0.85 * len(dataset))  # 90% train, 10% eval\n",
        "eval_size = len(dataset) - train_size\n",
        "train_dataset, eval_dataset = random_split(dataset, [train_size, eval_size])\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}, Eval size: {len(eval_dataset)}\")\n",
        "\n",
        "# Define a custom data collator that works with our dictionary format and removes extra keys\n",
        "def custom_data_collator(features):\n",
        "    # Stack pixel_values (they are already tensors)\n",
        "    pixel_values = torch.stack([f[\"pixel_values\"] for f in features])\n",
        "\n",
        "    # Pad labels dynamically using pad_sequence\n",
        "    labels = [f[\"labels\"] for f in features]\n",
        "    padded_labels = torch.nn.utils.rnn.pad_sequence(\n",
        "        labels, batch_first=True, padding_value=-100\n",
        "    )\n",
        "\n",
        "    batch = {\"pixel_values\": pixel_values, \"labels\": padded_labels}\n",
        "    return batch\n",
        "\n",
        "def data_collator_wrapper(features):\n",
        "    batch = custom_data_collator(features)\n",
        "    # Remove any keys that might have been added by the Trainer; for example:\n",
        "    batch.pop(\"num_items_in_batch\", None)\n",
        "    return batch\n",
        "\n",
        "# Training Arguments\n",
        "\n",
        "\n",
        "class MySeq2SeqTrainer(Seq2SeqTrainer):\n",
        "    def _prepare_inputs_for_model(self, inputs):\n",
        "        # Only keep keys that the model's forward function accepts.\n",
        "        allowed_keys = {\"pixel_values\", \"labels\"}\n",
        "        filtered_inputs = {k: v for k, v in inputs.items() if k in allowed_keys}\n",
        "        return super()._prepare_inputs_for_model(filtered_inputs)\n",
        "\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=CHECKPOINT_DIR,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,  # Keep eval batch size low\n",
        "    gradient_accumulation_steps=4,  # Helps compensate for small batch size\n",
        "    fp16=True,  # Saves VRAM using mixed precision\n",
        "    num_train_epochs=5,  # Avoid excessive memory usage over long runs\n",
        "    logging_steps=500,\n",
        "    evaluation_strategy=\"no\",  # **Best way to prevent crashes during eval**\n",
        "    save_steps=1500,  # **Less frequent saves to reduce memory spikes**\n",
        "    save_strategy=\"steps\",\n",
        "    save_total_limit=1,  # Keep only **1 checkpoint** to reduce storage\n",
        "    save_on_each_node=False,\n",
        "    load_best_model_at_end=False,  # **Disable to avoid extra VRAM use**\n",
        "    metric_for_best_model=\"loss\",\n",
        "    greater_is_better=False,\n",
        "    report_to=\"none\",\n",
        "    gradient_checkpointing=True,  # Helps save VRAM at the cost of slower training\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "# Trainer using the custom collator and custom trainer class\n",
        "trainer = MySeq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=custom_data_collator,  # Our previously defined collator\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "\n",
        "# Check for an existing checkpoint (if any)\n",
        "checkpoint_path = max(glob.glob(f\"{CHECKPOINT_DIR}/checkpoint-*\"), default=None)\n",
        "if checkpoint_path:\n",
        "    print(f\"Resuming from checkpoint: {checkpoint_path}\")\n",
        "else:\n",
        "    print(\"No checkpoint found. Starting training from scratch.\")\n",
        "\n",
        "# Train with checkpointing\n",
        "trainer.train(resume_from_checkpoint=checkpoint_path if checkpoint_path else None)\n"
      ],
      "metadata": {
        "id": "ygcTbkRNWHg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After training is complete:\n",
        "from transformers import ViTFeatureExtractor\n",
        "\n",
        "# 1. Save your trained model and tokenizer\n",
        "model_save_path = \"/content/drive/MyDrive/LatexNewModel\"\n",
        "model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "# 2. Save the feature extractor to the same directory\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(\"microsoft/trocr-base-stage1\")\n",
        "feature_extractor.save_pretrained(model_save_path)"
      ],
      "metadata": {
        "id": "fDMxCj-LlYxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the complete model\n",
        "from transformers import VisionEncoderDecoderModel, PreTrainedTokenizerFast, ViTFeatureExtractor\n",
        "\n",
        "model_save_path = \"/content/drive/MyDrive/LatexNewModel\"\n",
        "model = VisionEncoderDecoderModel.from_pretrained(model_save_path)\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_save_path)\n",
        "processor = ViTFeatureExtractor.from_pretrained(model_save_path)"
      ],
      "metadata": {
        "id": "KyBITsImngv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from PIL import Image\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# class HandwrittenLatexOCR:\n",
        "#     def __init__(self, model, feature_extractor, tokenizer, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "#         self.model = model.to(device)\n",
        "#         self.feature_extractor = feature_extractor\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.device = device\n",
        "#         self.model.eval()\n",
        "\n",
        "#     def preprocess_image(self, image):\n",
        "#         if isinstance(image, str):\n",
        "#             image = Image.open(image).convert('RGB')\n",
        "#         elif not isinstance(image, Image.Image):\n",
        "#             raise ValueError(\"Input must be a PIL Image or path to image\")\n",
        "\n",
        "#         pixel_values = self.feature_extractor(image, return_tensors=\"pt\").pixel_values\n",
        "#         return pixel_values.to(self.device), image\n",
        "\n",
        "#     @torch.no_grad()\n",
        "#     def __call__(self, image, max_length=15):  # Significantly reduced max_length\n",
        "#         pixel_values, original_image = self.preprocess_image(image)\n",
        "\n",
        "#         # More conservative generation parameters\n",
        "#         outputs = self.model.generate(\n",
        "#             pixel_values,\n",
        "#             max_length=max_length,\n",
        "#             num_beams=5,  # Increased slightly for better search\n",
        "#             early_stopping=True,\n",
        "#             pad_token_id=self.tokenizer.pad_token_id,\n",
        "#             eos_token_id=self.tokenizer.eos_token_id,\n",
        "#             bos_token_id=self.model.config.decoder_start_token_id,\n",
        "#             use_cache=True,\n",
        "#             length_penalty=1.0,  # Neutral length penalty\n",
        "#             no_repeat_ngram_size=1,  # Prevent immediate repetition\n",
        "#             repetition_penalty=3.0,  # Strong repetition penalty\n",
        "#             temperature=0.7,  # More conservative sampling\n",
        "#             top_k=20,  # More restricted top-k\n",
        "#             do_sample=False  # Disable sampling for more deterministic output\n",
        "#         )\n",
        "\n",
        "#         predicted_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "#         # Clean up common issues in the output\n",
        "#         predicted_text = self._clean_prediction(predicted_text)\n",
        "\n",
        "#         plt.figure(figsize=(8, 6))\n",
        "#         plt.imshow(original_image)\n",
        "#         plt.axis('off')\n",
        "#         plt.show()\n",
        "#         print(f\"Predicted LaTeX: {predicted_text}\")\n",
        "\n",
        "#         return predicted_text\n",
        "\n",
        "#     def _clean_prediction(self, text):\n",
        "#         \"\"\"Clean up common issues in predictions\"\"\"\n",
        "#         # Remove multiple spaces\n",
        "#         text = ' '.join(text.split())\n",
        "\n",
        "#         # Remove repeated symbols\n",
        "#         prev_char = None\n",
        "#         cleaned = []\n",
        "#         for char in text.split(','):\n",
        "#             if char != prev_char:\n",
        "#                 cleaned.append(char)\n",
        "#                 prev_char = char\n",
        "#         text = ','.join(cleaned)\n",
        "\n",
        "#         # Remove unnecessary spaces around operators\n",
        "#         text = text.replace(' , ', ', ')\n",
        "#         text = text.replace(' \\\\', '\\\\')\n",
        "\n",
        "#         return text.strip()\n",
        "\n",
        "#     def batch_inference(self, images, batch_size=8, max_length=15):\n",
        "#         predictions = []\n",
        "#         for i in range(0, len(images), batch_size):\n",
        "#             batch = images[i:i + batch_size]\n",
        "\n",
        "#             if isinstance(batch[0], str):\n",
        "#                 batch = [Image.open(img).convert('RGB') for img in batch]\n",
        "\n",
        "#             pixel_values = self.feature_extractor(batch, return_tensors=\"pt\").pixel_values\n",
        "#             pixel_values = pixel_values.to(self.device)\n",
        "\n",
        "#             with torch.no_grad():\n",
        "#                 outputs = self.model.generate(\n",
        "#                     pixel_values,\n",
        "#                     max_length=max_length,\n",
        "#                     num_beams=5,\n",
        "#                     early_stopping=True,\n",
        "#                     pad_token_id=self.tokenizer.pad_token_id,\n",
        "#                     eos_token_id=self.tokenizer.eos_token_id,\n",
        "#                     bos_token_id=self.model.config.decoder_start_token_id,\n",
        "#                     use_cache=True,\n",
        "#                     length_penalty=1.0,\n",
        "#                     no_repeat_ngram_size=1,\n",
        "#                     repetition_penalty=3.0,\n",
        "#                     temperature=0.7,\n",
        "#                     top_k=20,\n",
        "#                     do_sample=False\n",
        "#                 )\n",
        "\n",
        "#             batch_predictions = [\n",
        "#                 self._clean_prediction(self.tokenizer.decode(output, skip_special_tokens=True))\n",
        "#                 for output in outputs\n",
        "#             ]\n",
        "\n",
        "#             for img, pred in zip(batch, batch_predictions):\n",
        "#                 plt.figure(figsize=(8, 6))\n",
        "#                 plt.imshow(img)\n",
        "#                 plt.axis('off')\n",
        "#                 plt.show()\n",
        "#                 print(f\"Predicted LaTeX: {pred}\")\n",
        "#                 print(\"-\" * 50)\n",
        "\n",
        "#             predictions.extend(batch_predictions)\n",
        "\n",
        "#         return predictions"
      ],
      "metadata": {
        "id": "bXjCOizfpQeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class HandwrittenLatexOCR:\n",
        "    def __init__(self, model, feature_extractor, tokenizer, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.model = model.to(device)\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "        self.model.eval()\n",
        "\n",
        "    def preprocess_image(self, image):\n",
        "        if isinstance(image, str):\n",
        "            image = Image.open(image).convert('RGB')\n",
        "        elif not isinstance(image, Image.Image):\n",
        "            raise ValueError(\"Input must be a PIL Image or path to image\")\n",
        "        pixel_values = self.feature_extractor(image, return_tensors=\"pt\").pixel_values\n",
        "        return pixel_values.to(self.device), image\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(self, image, max_length=12):  # Increased max_length\n",
        "        pixel_values, original_image = self.preprocess_image(image)\n",
        "\n",
        "        # Modified generation parameters\n",
        "        outputs = self.model.generate(\n",
        "            pixel_values,\n",
        "            max_length=max_length,\n",
        "            num_beams=8,              # Increase beams for better search\n",
        "            early_stopping=True,\n",
        "            pad_token_id=self.tokenizer.pad_token_id,\n",
        "            eos_token_id=self.tokenizer.eos_token_id,\n",
        "            bos_token_id=self.model.config.decoder_start_token_id,\n",
        "            use_cache=True,\n",
        "            length_penalty=1.0,\n",
        "            no_repeat_ngram_size=2,   # Allow 2-gram repetition prevention\n",
        "            repetition_penalty=1.2,   # Lower repetition penalty\n",
        "            temperature=1.0,          # More diversity in sampling\n",
        "            top_k=50,                 # Increase top_k for a wider candidate set\n",
        "            top_p=0.95,               # Enable nucleus sampling\n",
        "            do_sample=False          # Enable sampling for more natural outputs\n",
        "        )\n",
        "\n",
        "        predicted_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Clean up prediction using a simpler cleaning function\n",
        "        predicted_text = self._clean_prediction(predicted_text)\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.imshow(original_image)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "        print(f\"Predicted LaTeX: {predicted_text}\")\n",
        "\n",
        "        return predicted_text\n",
        "\n",
        "    def _clean_prediction(self, text):\n",
        "        \"\"\"Simplify post-processing: remove extra spaces and accidental spacing before punctuation.\"\"\"\n",
        "        # Remove multiple spaces\n",
        "        text = ' '.join(text.split())\n",
        "        # Remove spaces before common punctuation (e.g., commas, periods, backslashes)\n",
        "        for punct in [',', '.', '\\\\', '{', '}', '(', ')']:\n",
        "            text = text.replace(\" \" + punct, punct)\n",
        "        return text.strip()\n",
        "\n",
        "    def batch_inference(self, images, batch_size=8, max_length=25):\n",
        "        predictions = []\n",
        "        for i in range(0, len(images), batch_size):\n",
        "            batch = images[i:i + batch_size]\n",
        "            if isinstance(batch[0], str):\n",
        "                batch = [Image.open(img).convert('RGB') for img in batch]\n",
        "            pixel_values = self.feature_extractor(batch, return_tensors=\"pt\").pixel_values.to(self.device)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    pixel_values,\n",
        "                    max_length=max_length,\n",
        "                    num_beams=8,\n",
        "                    early_stopping=True,\n",
        "                    pad_token_id=self.tokenizer.pad_token_id,\n",
        "                    eos_token_id=self.tokenizer.eos_token_id,\n",
        "                    bos_token_id=self.model.config.decoder_start_token_id,\n",
        "                    use_cache=True,\n",
        "                    length_penalty=1.0,\n",
        "                    no_repeat_ngram_size=2,\n",
        "                    repetition_penalty=1.2,\n",
        "                    temperature=1.0,\n",
        "                    top_k=50,\n",
        "                    top_p=0.95,\n",
        "                    do_sample=True\n",
        "                )\n",
        "            batch_predictions = [\n",
        "                self._clean_prediction(self.tokenizer.decode(output, skip_special_tokens=True))\n",
        "                for output in outputs\n",
        "            ]\n",
        "            for img, pred in zip(batch, batch_predictions):\n",
        "                plt.figure(figsize=(8, 6))\n",
        "                plt.imshow(img)\n",
        "                plt.axis('off')\n",
        "                plt.show()\n",
        "                print(f\"Predicted LaTeX: {pred}\")\n",
        "                print(\"-\" * 50)\n",
        "            predictions.extend(batch_predictions)\n",
        "        return predictions\n"
      ],
      "metadata": {
        "id": "14Ro9ABBjmlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ocr = HandwrittenLatexOCR(model, feature_extractor, tokenizer)\n",
        "\n",
        "# Single image inference\n",
        "image_path = \"/content/drive/MyDrive/121_em_328.inkml.png\"\n",
        "prediction = ocr(image_path)\n",
        "\n",
        "# Batch inference\n",
        "# image_paths = [\"image1.png\", \"image2.png\", \"image3.png\"]\n",
        "# predictions = ocr.batch_inference(image_paths)"
      ],
      "metadata": {
        "id": "RJAk0NGDp2sU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}